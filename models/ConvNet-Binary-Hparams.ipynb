{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d709e478",
   "metadata": {},
   "source": [
    "## Model Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91542f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "DATASET_SIZE = 1500\n",
    "MAX_EPOCHS = 20\n",
    "K_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bfede4",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c6945d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.15.5\n",
      "Tensorboard Version: 1.15.0\n",
      "GPU Found: True\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "%reload_ext tensorboard\n",
    "import tensorboard\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import time\n",
    "\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "print(\"Tensorboard Version:\", tensorboard.__version__)\n",
    "print(\"GPU Found:\", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd3b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([64, 128]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.2, 0.5]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'rmsprop']))\n",
    "HP_L_RATE= hp.HParam('learning_rate', hp.Discrete([0.0005, 0.001]))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.compat.v1.summary.FileWriter('./logs/hparam_tuning'):\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER, HP_L_RATE],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5197d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in train data\n",
    "train_df = pd.read_csv('../covidx-cxr2/train_COVIDx9B.txt', \n",
    "                       sep=\" \", header=None)\n",
    "# add columns to go from 0, 1, 2, 3 to patient id, filename, class etc\n",
    "train_df.columns=['patient id', 'filename', 'class', 'data source']\n",
    "# drop patient id and datasource as not needed\n",
    "train_df=train_df.drop(['patient id', 'data source'], axis=1 )\n",
    "\n",
    "# read in test data\n",
    "test_df = pd.read_csv('../covidx-cxr2/test_COVIDx9B.txt', \n",
    "                      sep=\" \", header=None)\n",
    "# add columns to go from 0, 1, 2, 3 to patient id, filename, class etc\n",
    "test_df.columns=['patient id', 'filename', 'class', 'data source']\n",
    "# drop patient id and datasource as not needed\n",
    "test_df=test_df.drop(['patient id', 'data source'], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe14addd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts:\n",
      "COVID-19     16490\n",
      "normal        8085\n",
      "pneumonia     5555\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Test class counts:\n",
      "COVID-19     200\n",
      "normal       100\n",
      "pneumonia    100\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train class counts:\")\n",
    "print(train_df['class'].value_counts())\n",
    "print(\"\\nTest class counts:\")\n",
    "print(test_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e42bf5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts:\n",
      "COVID-19     533\n",
      "pneumonia    533\n",
      "normal       533\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "negative  = train_df[train_df['class']=='negative']   # normal values in class column\n",
    "positive = train_df[train_df['class']=='positive']  # COVID-19 values in class column\n",
    "\n",
    "from sklearn.utils import resample\n",
    "# downsample training data to 400 values of each class, to reduce class bias and reduce training time\n",
    "\n",
    "df_negative_downsampled = resample(negative, replace = True, n_samples = DATASET_SIZE//2)\n",
    "df_positive_downsampled = resample(positive, replace = True, n_samples = DATASET_SIZE//2) \n",
    "\n",
    "#concatenate\n",
    "train_df = pd.concat([df_negative_downsampled, df_positive_downsampled])\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "train_df = shuffle(train_df) # shuffling so that there is particular sequence\n",
    "print(\"Train class counts:\")\n",
    "print(train_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7caf2941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts:\n",
      "COVID-19     485\n",
      "pneumonia    483\n",
      "normal       471\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Valid class counts:\n",
      "normal       62\n",
      "pneumonia    50\n",
      "COVID-19     48\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Test class counts:\n",
      "COVID-19     200\n",
      "normal       100\n",
      "pneumonia    100\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train class counts:\")\n",
    "print(train_df['class'].value_counts())\n",
    "print(\"\\nTest class counts:\")\n",
    "print(test_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b51c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess images\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "test_gen = test_datagen.flow_from_dataframe(dataframe = test_df, directory=\"../covidx-cxr2/test\", x_col='filename', \n",
    "                                            y_col='class', target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE, \n",
    "                                            color_mode='rgb', class_mode='binary', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7105a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1439 validated image filenames belonging to 3 classes.\n",
      "Found 160 validated image filenames belonging to 3 classes.\n",
      "Found 400 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_images_cv(train_index, val_index):\n",
    "    training_data = train_df.iloc[train_index]\n",
    "    validation_data = train_df.iloc[val_index]\n",
    "    train_datagen = ImageDataGenerator(rotation_range = 20, width_shift_range = 0.2, height_shift_range = 0.2, \n",
    "                                       shear_range = 0.2, zoom_range = 0.1, horizontal_flip = True)\n",
    "\n",
    "    #Now fit the them to get the images from directory (name of the images are given in dataframe) with augmentation\n",
    "\n",
    "    train_gen = train_datagen.flow_from_dataframe(dataframe = training_data, directory=\"../covidx-cxr2/train\", x_col='filename', \n",
    "                                                  y_col='class', target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE, \n",
    "                                                  color_mode='rgb', class_mode='binary')\n",
    "    valid_gen = test_datagen.flow_from_dataframe(dataframe = validation_data, directory=\"../covidx-cxr2/train\", x_col='filename',\n",
    "                                                 y_col='class', target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE, \n",
    "                                                 color_mode='rgb', class_mode='binary')\n",
    "    return train_gen, valid_gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a4efa7",
   "metadata": {},
   "source": [
    "## ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2cb5dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries for ConvNet\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f037e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hparams):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape = (IMAGE_SIZE,IMAGE_SIZE,3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(hparams[HP_NUM_UNITS]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(hparams[HP_DROPOUT]))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    optimizer_name = hparams[HP_OPTIMIZER]\n",
    "    learning_rate = hparams[HP_L_RATE]\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"unexpected optimizer name: %r\" % (optimizer_name,))\n",
    "\n",
    "\n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a055b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(k):\n",
    "    return str(\"ConvNet-Binary-\" + \"Fold-\"+str(k)+\"-\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494197d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 198, 198, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 198, 198, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 99, 99, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 97, 97, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 97, 97, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 46, 46, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 33856)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                2166848   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 2,195,683\n",
      "Trainable params: 2,195,683\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# perform HP tuning and CV on the model on the data\n",
    "Y = train_df['class']\n",
    "trained_models = []\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "    for dropout_rate in HP_DROPOUT.domain.values:\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "            for learning_rate in HP_L_RATE.domain.values:\n",
    "                \n",
    "                skf = StratifiedKFold(n_splits = K_FOLDS, shuffle = True) \n",
    "                k_i = 0\n",
    "                k_fold_models = []\n",
    "                elapsed_times = []\n",
    "                hparams = {\n",
    "                    HP_NUM_UNITS: num_units,\n",
    "                    HP_DROPOUT: dropout_rate,\n",
    "                    HP_OPTIMIZER: optimizer,\n",
    "                    HP_L_RATE: learning_rate\n",
    "                }\n",
    "                print('HPARAMS:',{h.name: hparams[h] for h in hparams})\n",
    "                \n",
    "                for train_index, val_index in skf.split(np.zeros(len(Y)),Y):\n",
    "                    print('--- Starting trial: %s' % k_i)\n",
    "                    t = time.process_time()\n",
    "                    train_gen, valid_gen = preprocess_images_cv(train_index, val_index)\n",
    "\n",
    "                    logdir = os.path.join('logs','fit', get_model_name(k_i))\n",
    "                    hpdir = os.path.join('logs','hparam_tuning', get_model_name(k_i))\n",
    "                    modeldir = os.path.join('trained classifiers',str(get_model_name(k_i)+\".h5\"))\n",
    "                    callbacks = [\n",
    "                        keras.callbacks.ModelCheckpoint(modeldir, save_best_only=True, verbose = 0),\n",
    "                        keras.callbacks.EarlyStopping(patience=3, monitor='val_loss', verbose=1, restore_best_weights=True),\n",
    "                        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, verbose=1),\n",
    "                        keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "                      ]\n",
    "                   \n",
    "                    hp.hparams(hparams)  # record the values used in this trial\n",
    "                    model = create_model(hparams)\n",
    "                    model.fit(train_gen, validation_data=valid_gen, epochs=MAX_EPOCHS, callbacks=callbacks)\n",
    "                    elapsed_times.append(time.process_time() - t)\n",
    "                    k_fold_models.append(modeldir)\n",
    "                    \n",
    "                    del model, train_gen, valid_gen\n",
    "                    keras.backend.clear_session()\n",
    "                    k_i += 1\n",
    "                    \n",
    "                trained_models.append([hparams, k_fold_models, elapsed_times])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58edf5c0",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e0890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42f92a11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 22s 3s/step - loss: 0.7708 - acc: 0.6950\n",
      "\n",
      "EVALUATION ON TRAIN DATA\n",
      "Train loss: 0.7627712583028251\n",
      "Train acc: 0.6421126\n",
      "\n",
      "EVALUATION ON VALIDATION DATA\n",
      "Val loss: 0.7240696748097738\n",
      "Val acc: 0.725\n",
      "\n",
      "EVALUATION ON TEST DATA\n",
      "Test loss: 0.7708391376904079\n",
      "Test acc: 0.695\n"
     ]
    }
   ],
   "source": [
    "for j in trained_models:\n",
    "    model = create_model(j[0])\n",
    "    accuracies = []\n",
    "    k_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    for i in j[1]:\n",
    "        model.load_weights(i)\n",
    "        test_pred = model.predict(test_gen)\n",
    "        y_pred = np.rint(test_pred).flatten()\n",
    "        y_test= [test_gen.class_indices[k] for k in test_df['class'].values.tolist()]\n",
    "        k_accuracies.append(model.evaluate(test_gen)[1])\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "        recalls.append(recall_score(y_test, y_pred))\n",
    "        f1s.append(f1_score(y_test, y_pred))\n",
    "\n",
    "    print(\"SCORE FOR HPARAMS:\", {h.name: j[0][h] for h in j[0]})\n",
    "    print(\"Mean accuracy:\", mean(accuracies))\n",
    "    print(\"Mean k_accuracy;\", mean(k_accuracies))\n",
    "    print(\"Mean precision:\", mean(precisions))\n",
    "    print(\"Mean recall:\", mean(recalls))\n",
    "    print(\"Mean f1:\", mean(f1s))\n",
    "    print(\"Mean training time:\", mean(j[2]))\n",
    "    hdict = {h.name: j[0][h] for h in j[0]}\n",
    "    df =  pd.DataFrame(np.array([[str(\"ConvNet-\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
    "                                  hdict.get('num_units'), hdict.get('dropout'), hdict.get('optimizer'), \n",
    "                                  hdict.get('learning_rate'),IMAGE_SIZE, BATCH_SIZE, DATASET_SIZE, MAX_EPOCHS, K_FOLDS,\n",
    "                                  mean(accuracies), mean(k_accuracies), mean(precisions), mean(recalls), mean(f1s), mean(j[2])\n",
    "                                 ]]),\n",
    "                       columns=['Model Name','num_units', 'dropout','optimizer','learning_rate',\n",
    "                                'IMAGE_SIZE', 'BATCH_SIZE', 'DATASET_SIZE', 'MAX_EPOCHS',\n",
    "                                'K_FOLDS','Accuracy', 'K_accuracy', 'Precision', 'Recall', 'f1 Score', 'Training Time'])\n",
    "    with open('results.csv', 'a') as f:\n",
    "        df.to_csv(f, mode='a', header=f.tell()==0, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16082b",
   "metadata": {},
   "source": [
    "NOTE: Accuracy and loss graphs can be seen on TensorBoard, run \"%tensorboard --logdir logs --host 0.0.0.0\" to open"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
