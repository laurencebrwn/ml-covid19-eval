{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3345cd-9539-4c92-98d7-38c6b3e22790",
   "metadata": {},
   "source": [
    "## Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac0dff3-a4bd-4672-bf59-6c05ba05909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp -r gs://covidx-bucket/covidx-cxr2/ ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fe73b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e6ff422",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "DATASET_SIZE = 1500\n",
    "MAX_EPOCHS = 20\n",
    "K_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bfede4",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c6945d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.8.0\n",
      "Tensorboard Version: 2.8.0\n",
      "WARNING:tensorflow:From /tmp/ipykernel_1/3180827155.py:19: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU Found: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 13:35:59.920838: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-24 13:35:59.932467: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-03-24 13:35:59.932509: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-24 13:35:59.932550: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (vm-4c2cc432-9ead-4e1e-8f11-a56127480af4): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "%reload_ext tensorboard\n",
    "import tensorboard\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import time\n",
    "\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "print(\"Tensorboard Version:\", tensorboard.__version__)\n",
    "print(\"GPU Found:\", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5aa799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete(['N/A']))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.1, 0.2]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "HP_L_RATE= hp.HParam('learning_rate', hp.Discrete([0.0005, 0.001]))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('trained-classifiers/logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER, HP_L_RATE],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5197d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in train data\n",
    "train_df = pd.read_csv('covidx-cxr2/train_COVIDx9B.txt', \n",
    "                       sep=\" \", header=None)\n",
    "# add columns to go from 0, 1, 2, 3 to patient id, filename, class etc\n",
    "train_df.columns=['patient id', 'filename', 'class', 'data source']\n",
    "# drop patient id and datasource as not needed\n",
    "train_df=train_df.drop(['patient id', 'data source'], axis=1 )\n",
    "\n",
    "# read in test data\n",
    "test_df = pd.read_csv('covidx-cxr2/test_COVIDx9B.txt', \n",
    "                      sep=\" \", header=None)\n",
    "# add columns to go from 0, 1, 2, 3 to patient id, filename, class etc\n",
    "test_df.columns=['patient id', 'filename', 'class', 'data source']\n",
    "# drop patient id and datasource as not needed\n",
    "test_df=test_df.drop(['patient id', 'data source'], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe14addd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts:\n",
      "positive    16490\n",
      "negative    13992\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Test class counts:\n",
      "positive    200\n",
      "negative    200\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train class counts:\")\n",
    "print(train_df['class'].value_counts())\n",
    "print(\"\\nTest class counts:\")\n",
    "print(test_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e42bf5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts:\n",
      "positive    750\n",
      "negative    750\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "negative  = train_df[train_df['class']=='negative']   # normal values in class column\n",
    "positive = train_df[train_df['class']=='positive']  # COVID-19 values in class column\n",
    "\n",
    "from sklearn.utils import resample\n",
    "# downsample training data to 400 values of each class, to reduce class bias and reduce training time\n",
    "\n",
    "df_negative_downsampled = resample(negative, replace = True, n_samples = DATASET_SIZE//2)\n",
    "df_positive_downsampled = resample(positive, replace = True, n_samples = DATASET_SIZE//2) \n",
    "\n",
    "#concatenate\n",
    "train_df = pd.concat([df_negative_downsampled, df_positive_downsampled])\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "train_df = shuffle(train_df) # shuffling so that there is particular sequence\n",
    "print(\"Train class counts:\")\n",
    "print(train_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7caf2941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts:\n",
      "positive    750\n",
      "negative    750\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Test class counts:\n",
      "positive    200\n",
      "negative    200\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train class counts:\")\n",
    "print(train_df['class'].value_counts())\n",
    "print(\"\\nTest class counts:\")\n",
    "print(test_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a7105a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# preprocess images\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "test_gen = test_datagen.flow_from_dataframe(dataframe = test_df, directory=\"covidx-cxr2/test\", x_col='filename', \n",
    "                                            y_col='class', target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE, \n",
    "                                            color_mode='rgb', class_mode='binary', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc1d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images_cv(train_index, val_index):\n",
    "    training_data = train_df.iloc[train_index]\n",
    "    validation_data = train_df.iloc[val_index]\n",
    "    train_datagen = ImageDataGenerator(rotation_range = 20, width_shift_range = 0.2, height_shift_range = 0.2, \n",
    "                                       shear_range = 0.2, zoom_range = 0.1, horizontal_flip = True)\n",
    "\n",
    "    #Now fit the them to get the images from directory (name of the images are given in dataframe) with augmentation\n",
    "\n",
    "    train_gen = train_datagen.flow_from_dataframe(dataframe = training_data, directory=\"covidx-cxr2/train\", x_col='filename', \n",
    "                                                  y_col='class', target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE, \n",
    "                                                  color_mode='rgb', class_mode='binary')\n",
    "    valid_gen = test_datagen.flow_from_dataframe(dataframe = validation_data, directory=\"covidx-cxr2/train\", x_col='filename',\n",
    "                                                 y_col='class', target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE, \n",
    "                                                 color_mode='rgb', class_mode='binary')\n",
    "    return train_gen, valid_gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4174e3f4",
   "metadata": {},
   "source": [
    "## SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96ce3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries for ResNet\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0d52d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire(x, squeeze, expand):\n",
    "    y  = keras.layers.Conv2D(filters=squeeze, kernel_size=1, activation='relu', padding='same')(x)\n",
    "    y = keras.layers.BatchNormalization(momentum=0.9)(y)\n",
    "    y1 = keras.layers.Conv2D(filters=expand//2, kernel_size=1, activation='relu', padding='same')(y)\n",
    "    y1 = keras.layers.BatchNormalization(momentum=0.9)(y1)\n",
    "    y3 = keras.layers.Conv2D(filters=expand//2, kernel_size=3, activation='relu', padding='same')(y)\n",
    "    y3 = keras.layers.BatchNormalization(momentum=0.9)(y3)\n",
    "    return keras.layers.concatenate([y1, y3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "080c82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire_module(squeeze, expand):\n",
    "  return lambda x: fire(x, squeeze, expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "836aa1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hparams):\n",
    "    x = keras.layers.Input(shape=[IMAGE_SIZE, IMAGE_SIZE, 3]) \n",
    "\n",
    "    y = keras.layers.Conv2D(kernel_size=3, filters=32, padding='same', use_bias=True, activation='relu')(x)\n",
    "    y = keras.layers.BatchNormalization(momentum=0.9)(y)\n",
    "    y = fire_module(24, 48)(y)\n",
    "    y = keras.layers.MaxPooling2D(pool_size=2)(y)\n",
    "    y = fire_module(48, 96)(y)\n",
    "    y = keras.layers.MaxPooling2D(pool_size=2)(y)\n",
    "    y = fire_module(64, 128)(y)\n",
    "    y = keras.layers.MaxPooling2D(pool_size=2)(y)\n",
    "    y = fire_module(48, 96)(y)\n",
    "    y = keras.layers.MaxPooling2D(pool_size=2)(y)\n",
    "    y = fire_module(24, 48)(y)\n",
    "    y = keras.layers.GlobalAveragePooling2D()(y)\n",
    "    # Add dropout to reduce overfitting\n",
    "    y = keras.layers.Dropout(hparams[HP_DROPOUT])(y)\n",
    "    y = keras.layers.Dense(1, activation='sigmoid')(y)\n",
    "\n",
    "    model = tf.keras.Model(x, y)\n",
    "\n",
    "    optimizer_name = hparams[HP_OPTIMIZER]\n",
    "    learning_rate = hparams[HP_L_RATE]\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"unexpected optimizer name: %r\" % (optimizer_name,))\n",
    "\n",
    "\n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1a25146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(k):\n",
    "    return str(\"SqueezeNet-Binary-\" + \"Fold-\"+str(k)+\"-\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8faa2fa4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPARAMS: {'num_units': 'N/A', 'dropout': 0.1, 'optimizer': 'adam', 'learning_rate': 0.0005}\n",
      "--- Starting trial: 0\n",
      "Found 1200 validated image filenames belonging to 2 classes.\n",
      "Found 300 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/20\n",
      " 2/38 [>.............................] - ETA: 2:00 - loss: 0.6260 - accuracy: 0.6875"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/1761103439.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# record the values used in this trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0melapsed_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mk_fold_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeldir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# perform HP tuning and CV on the model on the data\n",
    "Y = train_df['class']\n",
    "trained_models = []\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "    for dropout_rate in HP_DROPOUT.domain.values:\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "            for learning_rate in HP_L_RATE.domain.values:\n",
    "                \n",
    "                skf = StratifiedKFold(n_splits = K_FOLDS, shuffle = True) \n",
    "                k_i = 0\n",
    "                k_fold_models = []\n",
    "                elapsed_times = []\n",
    "                hparams = {\n",
    "                    HP_NUM_UNITS: num_units,\n",
    "                    HP_DROPOUT: dropout_rate,\n",
    "                    HP_OPTIMIZER: optimizer,\n",
    "                    HP_L_RATE: learning_rate\n",
    "                }\n",
    "                print('HPARAMS:',{h.name: hparams[h] for h in hparams})\n",
    "                \n",
    "                for train_index, val_index in skf.split(np.zeros(len(Y)),Y):\n",
    "                    print('--- Starting trial: %s' % k_i)\n",
    "                    t = time.process_time()\n",
    "                    train_gen, valid_gen = preprocess_images_cv(train_index, val_index)\n",
    "\n",
    "                    logdir = os.path.join('trained classifiers','logs','fit', get_model_name(k_i))\n",
    "                    hpdir = os.path.join('trained classifiers','logs','hparam_tuning', get_model_name(k_i))\n",
    "                    modeldir = os.path.join('trained classifiers',str(get_model_name(k_i)+\".h5\"))\n",
    "                    callbacks = [\n",
    "                        keras.callbacks.ModelCheckpoint(modeldir, save_best_only=True, verbose = 0),\n",
    "                        keras.callbacks.EarlyStopping(patience=3, monitor='val_loss', verbose=1, restore_best_weights=True),\n",
    "                        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, verbose=1),\n",
    "                        keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "                      ]\n",
    "                   \n",
    "                    hp.hparams(hparams)  # record the values used in this trial\n",
    "                    model = create_model(hparams)\n",
    "                    model.fit(train_gen, validation_data=valid_gen, epochs=MAX_EPOCHS, callbacks=callbacks)\n",
    "                    elapsed_times.append(time.process_time() - t)\n",
    "                    k_fold_models.append(modeldir)\n",
    "                    \n",
    "                    del model, train_gen, valid_gen\n",
    "                    keras.backend.clear_session()\n",
    "                    k_i += 1\n",
    "                    \n",
    "                trained_models.append([hparams, k_fold_models, elapsed_times])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddf6f35",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69b88b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 21s 422ms/step - loss: 0.4077 - acc: 0.8725\n",
      "\n",
      "EVALUATION ON TRAIN DATA\n",
      "Train loss: 0.6255580113114707\n",
      "Train acc: 0.7553857\n",
      "\n",
      "EVALUATION ON VALIDATION DATA\n",
      "Val loss: 0.7708669796586036\n",
      "Val acc: 0.65000004\n",
      "\n",
      "EVALUATION ON TEST DATA\n",
      "Test loss: 0.40767769783735275\n",
      "Test acc: 0.8725\n"
     ]
    }
   ],
   "source": [
    "for j in trained_models:\n",
    "    model = create_model(j[0])\n",
    "    accuracies = []\n",
    "    k_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    tp = []\n",
    "    fp = []\n",
    "    tn = []\n",
    "    fn = []\n",
    "    for i in j[1]:\n",
    "        model.load_weights(i)\n",
    "        test_pred = model.predict(test_gen)\n",
    "        y_pred = np.rint(test_pred).flatten()\n",
    "        y_test= [test_gen.class_indices[k] for k in test_df['class'].values.tolist()]\n",
    "        k_accuracies.append(model.evaluate(test_gen)[1])\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "        recalls.append(recall_score(y_test, y_pred))\n",
    "        f1s.append(f1_score(y_test, y_pred))\n",
    "        tn_x, fp_x, fn_x, tp_x = confusion_matrix(y_test, y_pred).ravel()\n",
    "        tp.append(tp_x)\n",
    "        fp.append(fp_x)\n",
    "        tn.append(tn_x)\n",
    "        fn.append(fn_x)\n",
    "\n",
    "    print(\"SCORE FOR HPARAMS:\", {h.name: j[0][h] for h in j[0]})\n",
    "    print(\"Mean accuracy:\", mean(accuracies))\n",
    "    print(\"Mean k_accuracy;\", mean(k_accuracies))\n",
    "    print(\"Mean precision:\", mean(precisions))\n",
    "    print(\"Mean recall:\", mean(recalls))\n",
    "    print(\"Mean f1:\", mean(f1s))\n",
    "    print(\"Mean training time:\", mean(j[2]))\n",
    "    hdict = {h.name: j[0][h] for h in j[0]}\n",
    "    df =  pd.DataFrame(np.array([[str(j[1]),\n",
    "                                  hdict.get('num_units'), hdict.get('dropout'), hdict.get('optimizer'), \n",
    "                                  hdict.get('learning_rate'),IMAGE_SIZE, BATCH_SIZE, DATASET_SIZE, MAX_EPOCHS, K_FOLDS,\n",
    "                                  mean(accuracies), mean(k_accuracies), mean(precisions), mean(recalls), mean(f1s),\n",
    "                                  mean(tn), mean(fp), mean(fn), mean(tp), mean(j[2])\n",
    "                                 ]]),\n",
    "                       columns=['Model Names','num_units', 'dropout','optimizer','learning_rate',\n",
    "                                'IMAGE_SIZE', 'BATCH_SIZE', 'DATASET_SIZE', 'MAX_EPOCHS',\n",
    "                                'K_FOLDS','Accuracy', 'K_accuracy', 'Precision', 'Recall', 'f1 Score', \n",
    "                                'True Negatives', ' False Positives', 'False Negatives', 'True Positives', 'Training Time'])\n",
    "    with open('trained-classifiers/Squeezenet.csv', 'a') as f:\n",
    "        df.to_csv(f, mode='a', header=f.tell()==0, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5722a458",
   "metadata": {},
   "source": [
    "NOTE: Accuracy and loss graphs can be seen on TensorBoard, run \"%tensorboard --logdir logs --host 0.0.0.0\" to open"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa216a0-71d9-4241-b193-2ca261c9a863",
   "metadata": {},
   "source": [
    "## Save Weights and Results to Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7d8da-5646-4b37-9bea-38d2d6864eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp -r trained-classifiers gs://trained-classifiers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca07e9b2-4dcc-4c3c-a056-713453c49839",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp -r model-checkpoints gs://trained-classifiers/"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu:latest"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
