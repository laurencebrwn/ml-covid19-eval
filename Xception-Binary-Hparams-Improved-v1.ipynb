{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c8916b3-febe-497a-89ca-b5e2275d582a",
   "metadata": {},
   "source": [
    "## Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f2b61d-3a56-473e-ad4d-72de3b097228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil -m cp -r gs://covidx-bucket/covidx-cxr2/ ./"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ccce8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fe9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "DATASET_SIZE = 1500\n",
    "MAX_EPOCHS = 20\n",
    "K_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bfede4",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c6945d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.15.5\n",
      "Tensorboard Version: 1.15.0\n",
      "GPU Found: True\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "%reload_ext tensorboard\n",
    "import tensorboard\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import time\n",
    "\n",
    "print(\"Tensorflow Version:\", tf.__version__)\n",
    "print(\"Tensorboard Version:\", tensorboard.__version__)\n",
    "print(\"GPU Found:\", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a26665",
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([128]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.1]))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "HP_L_RATE= hp.HParam('learning_rate', hp.Discrete([0.001]))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "# for tf2 use tf.summary.create_file_writer().as_default()\n",
    "\n",
    "with tf.compat.v1.summary.FileWriter('trained classifiers/logs/hparam_tuning'):\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER, HP_L_RATE],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5197d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in train data\n",
    "train_df = pd.read_csv('covidx-cxr2/train_COVIDx9B.txt', \n",
    "                       sep=\" \", header=None)\n",
    "# add columns to go from 0, 1, 2, 3 to patient id, filename, class etc\n",
    "train_df.columns=['patient id', 'filename', 'class', 'data source']\n",
    "# drop patient id and datasource as not needed\n",
    "train_df=train_df.drop(['patient id', 'data source'], axis=1 )\n",
    "\n",
    "# read in test data\n",
    "test_df = pd.read_csv('covidx-cxr2/test_COVIDx9B.txt', \n",
    "                      sep=\" \", header=None)\n",
    "# add columns to go from 0, 1, 2, 3 to patient id, filename, class etc\n",
    "test_df.columns=['patient id', 'filename', 'class', 'data source']\n",
    "# drop patient id and datasource as not needed\n",
    "test_df=test_df.drop(['patient id', 'data source'], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe14addd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts:\n",
      "positive    16490\n",
      "negative    13992\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Test class counts:\n",
      "positive    200\n",
      "negative    200\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train class counts:\")\n",
    "print(train_df['class'].value_counts())\n",
    "print(\"\\nTest class counts:\")\n",
    "print(test_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e42bf5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts:\n",
      "negative    750\n",
      "positive    750\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "negative  = train_df[train_df['class']=='negative']   # normal values in class column\n",
    "positive = train_df[train_df['class']=='positive']  # COVID-19 values in class column\n",
    "\n",
    "from sklearn.utils import resample\n",
    "# downsample training data to 400 values of each class, to reduce class bias and reduce training time\n",
    "\n",
    "df_negative_downsampled = resample(negative, replace = True, n_samples = DATASET_SIZE//2)\n",
    "df_positive_downsampled = resample(positive, replace = True, n_samples = DATASET_SIZE//2) \n",
    "\n",
    "#concatenate\n",
    "train_df = pd.concat([df_negative_downsampled, df_positive_downsampled])\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "train_df = shuffle(train_df) # shuffling so that there is particular sequence\n",
    "print(\"Train class counts:\")\n",
    "print(train_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7caf2941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts:\n",
      "negative    750\n",
      "positive    750\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Test class counts:\n",
      "positive    200\n",
      "negative    200\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train class counts:\")\n",
    "print(train_df['class'].value_counts())\n",
    "print(\"\\nTest class counts:\")\n",
    "print(test_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a7105a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# preprocess images\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.xception.preprocess_input)\n",
    "\n",
    "test_gen = test_datagen.flow_from_dataframe(dataframe = test_df, directory=\"covidx-cxr2/test\", x_col='filename', \n",
    "                                            y_col='class', target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE, \n",
    "                                            color_mode='rgb', class_mode='binary', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc520d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images_cv(train_index, val_index):\n",
    "    training_data = train_df.iloc[train_index]\n",
    "    validation_data = train_df.iloc[val_index]\n",
    "    train_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.xception.preprocess_input,\n",
    "                                       rotation_range = 20, width_shift_range = 0.2, height_shift_range = 0.2, \n",
    "                                       shear_range = 0.2, zoom_range = 0.1, horizontal_flip = True, \n",
    "                                       vertical_flip = True)\n",
    "\n",
    "    #Now fit the them to get the images from directory (name of the images are given in dataframe) with augmentation\n",
    "\n",
    "    train_gen = train_datagen.flow_from_dataframe(dataframe = training_data, directory=\"covidx-cxr2/train\", x_col='filename', \n",
    "                                                  y_col='class', target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE, \n",
    "                                                  color_mode='rgb', class_mode='binary')\n",
    "    valid_gen = test_datagen.flow_from_dataframe(dataframe = validation_data, directory=\"covidx-cxr2/train\", x_col='filename',\n",
    "                                                 y_col='class', target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE, \n",
    "                                                 color_mode='rgb', class_mode='binary')\n",
    "    return train_gen, valid_gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4174e3f4",
   "metadata": {},
   "source": [
    "## Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96ce3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries for ResNet\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "836aa1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hparams):\n",
    "    # create the base pre-trained model\n",
    "    base_model = keras.applications.Xception(weights='imagenet', input_shape = (IMAGE_SIZE,IMAGE_SIZE,3),\n",
    "                                                  include_top=False)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model = keras.Sequential([base_model,\n",
    "                              keras.layers.GlobalAveragePooling2D(),\n",
    "                              keras.layers.Dense(hparams[HP_NUM_UNITS], activation='relu',\n",
    "                                                 kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "                                                 bias_regularizer=regularizers.l2(1e-4),\n",
    "                                                 activity_regularizer=regularizers.l2(1e-5)),\n",
    "                              keras.layers.BatchNormalization(),\n",
    "                              keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "                              keras.layers.Dense(1, activation='sigmoid')\n",
    "                             ])\n",
    "    \n",
    "    optimizer_name = hparams[HP_OPTIMIZER]\n",
    "    learning_rate = hparams[HP_L_RATE]\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "    else:\n",
    "        raise ValueError(\"unexpected optimizer name: %r\" % (optimizer_name,))\n",
    "\n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8275562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(k):\n",
    "    return str(\"Xception-Binary-\" + \"Fold-\"+str(k)+\"-\"+datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8faa2fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPARAMS: {'num_units': 128, 'dropout': 0.1, 'optimizer': 'adam', 'learning_rate': 0.001}\n",
      "--- Starting trial: 0\n",
      "Found 1200 validated image filenames belonging to 2 classes.\n",
      "Found 300 validated image filenames belonging to 2 classes.\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/20\n",
      "37/38 [============================>.] - ETA: 1s - loss: 0.5092 - acc: 0.8099Epoch 1/20\n",
      "38/38 [==============================] - 64s 2s/step - loss: 0.5102 - acc: 0.8075 - val_loss: 0.7472 - val_acc: 0.6200\n",
      "Epoch 2/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4216 - acc: 0.8724Epoch 1/20\n",
      "38/38 [==============================] - 26s 672ms/step - loss: 0.4240 - acc: 0.8692 - val_loss: 0.6193 - val_acc: 0.7700\n",
      "Epoch 3/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4374 - acc: 0.8562Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 12s - loss: 0.9084 - acc: 0.6367\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "38/38 [==============================] - 29s 752ms/step - loss: 0.4404 - acc: 0.8558 - val_loss: 0.9084 - val_acc: 0.6367\n",
      "Epoch 4/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4476 - acc: 0.8527Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 12s - loss: 0.6679 - acc: 0.7533\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "38/38 [==============================] - 26s 676ms/step - loss: 0.4450 - acc: 0.8533 - val_loss: 0.6679 - val_acc: 0.7533\n",
      "Epoch 5/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.3834 - acc: 0.8861Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 12s - loss: 0.7873 - acc: 0.7333Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "38/38 [==============================] - 29s 773ms/step - loss: 0.3803 - acc: 0.8875 - val_loss: 0.7873 - val_acc: 0.7333\n",
      "Epoch 00005: early stopping\n",
      "--- Starting trial: 1\n",
      "Found 1200 validated image filenames belonging to 2 classes.\n",
      "Found 300 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.5108 - acc: 0.8099Epoch 1/20\n",
      "38/38 [==============================] - 33s 868ms/step - loss: 0.5095 - acc: 0.8108 - val_loss: 0.8227 - val_acc: 0.5933\n",
      "Epoch 2/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4081 - acc: 0.8613Epoch 1/20\n",
      "38/38 [==============================] - 28s 747ms/step - loss: 0.4123 - acc: 0.8600 - val_loss: 0.7823 - val_acc: 0.6733\n",
      "Epoch 3/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4114 - acc: 0.8793Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 13s - loss: 0.7885 - acc: 0.6733\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "38/38 [==============================] - 29s 750ms/step - loss: 0.4075 - acc: 0.8817 - val_loss: 0.7885 - val_acc: 0.6733\n",
      "Epoch 4/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.3740 - acc: 0.8887Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 13s - loss: 0.7979 - acc: 0.6733\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "38/38 [==============================] - 29s 751ms/step - loss: 0.3718 - acc: 0.8900 - val_loss: 0.7979 - val_acc: 0.6733\n",
      "Epoch 5/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8810Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 12s - loss: 0.8488 - acc: 0.6667Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "38/38 [==============================] - 28s 739ms/step - loss: 0.3885 - acc: 0.8800 - val_loss: 0.8488 - val_acc: 0.6667\n",
      "Epoch 00005: early stopping\n",
      "--- Starting trial: 2\n",
      "Found 1200 validated image filenames belonging to 2 classes.\n",
      "Found 300 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.5177 - acc: 0.8348Epoch 1/20\n",
      "38/38 [==============================] - 35s 926ms/step - loss: 0.5165 - acc: 0.8342 - val_loss: 0.8893 - val_acc: 0.5333\n",
      "Epoch 2/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4453 - acc: 0.8579Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 12s - loss: 0.9250 - acc: 0.6367\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "38/38 [==============================] - 28s 730ms/step - loss: 0.4466 - acc: 0.8567 - val_loss: 0.9250 - val_acc: 0.6367\n",
      "Epoch 3/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4307 - acc: 0.8527Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 11s - loss: 0.9161 - acc: 0.6400\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "38/38 [==============================] - 28s 738ms/step - loss: 0.4293 - acc: 0.8525 - val_loss: 0.9161 - val_acc: 0.6400\n",
      "Epoch 4/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4129 - acc: 0.8664Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 11s - loss: 1.0076 - acc: 0.6367Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "38/38 [==============================] - 27s 715ms/step - loss: 0.4144 - acc: 0.8650 - val_loss: 1.0076 - val_acc: 0.6367\n",
      "Epoch 00004: early stopping\n",
      "--- Starting trial: 3\n",
      "Found 1200 validated image filenames belonging to 2 classes.\n",
      "Found 300 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.5348 - acc: 0.8125Epoch 1/20\n",
      "38/38 [==============================] - 31s 825ms/step - loss: 0.5293 - acc: 0.8150 - val_loss: 0.7518 - val_acc: 0.6700\n",
      "Epoch 2/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4074 - acc: 0.8759Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 10s - loss: 1.0532 - acc: 0.6033\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "38/38 [==============================] - 25s 652ms/step - loss: 0.4050 - acc: 0.8758 - val_loss: 1.0532 - val_acc: 0.6033\n",
      "Epoch 3/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8827Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 10s - loss: 1.0965 - acc: 0.6067\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "38/38 [==============================] - 26s 679ms/step - loss: 0.3742 - acc: 0.8842 - val_loss: 1.0965 - val_acc: 0.6067\n",
      "Epoch 4/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.3738 - acc: 0.8750Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 10s - loss: 1.1793 - acc: 0.6100Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "38/38 [==============================] - 26s 693ms/step - loss: 0.3715 - acc: 0.8758 - val_loss: 1.1793 - val_acc: 0.6100\n",
      "Epoch 00004: early stopping\n",
      "--- Starting trial: 4\n",
      "Found 1200 validated image filenames belonging to 2 classes.\n",
      "Found 300 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.5314 - acc: 0.8031Epoch 1/20\n",
      "38/38 [==============================] - 31s 820ms/step - loss: 0.5270 - acc: 0.8058 - val_loss: 1.0676 - val_acc: 0.5033\n",
      "Epoch 2/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.8699Epoch 1/20\n",
      "38/38 [==============================] - 26s 691ms/step - loss: 0.4098 - acc: 0.8683 - val_loss: 0.8664 - val_acc: 0.6067\n",
      "Epoch 3/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4260 - acc: 0.8587Epoch 1/20\n",
      "38/38 [==============================] - 28s 731ms/step - loss: 0.4288 - acc: 0.8583 - val_loss: 0.7887 - val_acc: 0.6333\n",
      "Epoch 4/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4066 - acc: 0.8647Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 10s - loss: 1.0675 - acc: 0.6400\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "38/38 [==============================] - 26s 687ms/step - loss: 0.4038 - acc: 0.8667 - val_loss: 1.0675 - val_acc: 0.6400\n",
      "Epoch 5/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.3855 - acc: 0.8784Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 10s - loss: 1.0322 - acc: 0.6600\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "38/38 [==============================] - 26s 679ms/step - loss: 0.3836 - acc: 0.8792 - val_loss: 1.0322 - val_acc: 0.6600\n",
      "Epoch 6/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.3452 - acc: 0.9050Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 11s - loss: 1.2009 - acc: 0.6567Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "38/38 [==============================] - 27s 699ms/step - loss: 0.3424 - acc: 0.9067 - val_loss: 1.2009 - val_acc: 0.6567\n",
      "Epoch 00006: early stopping\n",
      "HPARAMS: {'num_units': 128, 'dropout': 0.1, 'optimizer': 'sgd', 'learning_rate': 0.001}\n",
      "--- Starting trial: 0\n",
      "Found 1200 validated image filenames belonging to 2 classes.\n",
      "Found 300 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.7059 - acc: 0.6909Epoch 1/20\n",
      "38/38 [==============================] - 31s 816ms/step - loss: 0.7008 - acc: 0.6958 - val_loss: 0.7407 - val_acc: 0.6433\n",
      "Epoch 2/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4822 - acc: 0.8279Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 12s - loss: 0.7802 - acc: 0.5967\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "38/38 [==============================] - 24s 631ms/step - loss: 0.4809 - acc: 0.8283 - val_loss: 0.7802 - val_acc: 0.5967\n",
      "Epoch 3/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4652 - acc: 0.8527Epoch 1/20\n",
      "38/38 [==============================] - 26s 691ms/step - loss: 0.4621 - acc: 0.8533 - val_loss: 0.7305 - val_acc: 0.6467\n",
      "Epoch 4/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4503 - acc: 0.8493Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 12s - loss: 0.7825 - acc: 0.6267\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "38/38 [==============================] - 25s 653ms/step - loss: 0.4507 - acc: 0.8500 - val_loss: 0.7825 - val_acc: 0.6267\n",
      "Epoch 5/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4435 - acc: 0.8587Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 12s - loss: 0.8492 - acc: 0.5967\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "38/38 [==============================] - 25s 658ms/step - loss: 0.4394 - acc: 0.8600 - val_loss: 0.8492 - val_acc: 0.5967\n",
      "Epoch 6/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4510 - acc: 0.8493Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 11s - loss: 0.9469 - acc: 0.6000Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "38/38 [==============================] - 25s 671ms/step - loss: 0.4501 - acc: 0.8500 - val_loss: 0.9469 - val_acc: 0.6000\n",
      "Epoch 00006: early stopping\n",
      "--- Starting trial: 1\n",
      "Found 1200 validated image filenames belonging to 2 classes.\n",
      "Found 300 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.6675 - acc: 0.7063Epoch 1/20\n",
      "38/38 [==============================] - 31s 828ms/step - loss: 0.6632 - acc: 0.7083 - val_loss: 0.7804 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.5353 - acc: 0.8048Epoch 1/20\n",
      "38/38 [==============================] - 26s 685ms/step - loss: 0.5335 - acc: 0.8067 - val_loss: 0.7773 - val_acc: 0.5400\n",
      "Epoch 3/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4714 - acc: 0.8296Epoch 1/20\n",
      "38/38 [==============================] - 28s 725ms/step - loss: 0.4695 - acc: 0.8300 - val_loss: 0.7688 - val_acc: 0.5633\n",
      "Epoch 4/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4559 - acc: 0.8425Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 9s - loss: 0.8475 - acc: 0.5500 \n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "38/38 [==============================] - 26s 692ms/step - loss: 0.4563 - acc: 0.8425 - val_loss: 0.8475 - val_acc: 0.5500\n",
      "Epoch 5/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4248 - acc: 0.8587Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 9s - loss: 0.8181 - acc: 0.6100 \n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "38/38 [==============================] - 26s 682ms/step - loss: 0.4235 - acc: 0.8600 - val_loss: 0.8181 - val_acc: 0.6100\n",
      "Epoch 6/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4131 - acc: 0.8673Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 9s - loss: 0.8861 - acc: 0.5900 Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "38/38 [==============================] - 27s 699ms/step - loss: 0.4139 - acc: 0.8658 - val_loss: 0.8861 - val_acc: 0.5900\n",
      "Epoch 00006: early stopping\n",
      "--- Starting trial: 2\n",
      "Found 1200 validated image filenames belonging to 2 classes.\n",
      "Found 300 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.6949 - acc: 0.6909Epoch 1/20\n",
      "38/38 [==============================] - 31s 807ms/step - loss: 0.6887 - acc: 0.6942 - val_loss: 0.7621 - val_acc: 0.6267\n",
      "Epoch 2/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.5182 - acc: 0.8099Epoch 1/20\n",
      "38/38 [==============================] - 25s 671ms/step - loss: 0.5185 - acc: 0.8100 - val_loss: 0.7528 - val_acc: 0.6167\n",
      "Epoch 3/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4598 - acc: 0.8382Epoch 1/20\n",
      "38/38 [==============================] - 26s 691ms/step - loss: 0.4573 - acc: 0.8392 - val_loss: 0.7323 - val_acc: 0.6533\n",
      "Epoch 4/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4389 - acc: 0.8510Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 11s - loss: 0.7667 - acc: 0.6400\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "38/38 [==============================] - 25s 663ms/step - loss: 0.4442 - acc: 0.8483 - val_loss: 0.7667 - val_acc: 0.6400\n",
      "Epoch 5/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4272 - acc: 0.8639Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 11s - loss: 0.8064 - acc: 0.6167\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "38/38 [==============================] - 25s 654ms/step - loss: 0.4248 - acc: 0.8642 - val_loss: 0.8064 - val_acc: 0.6167\n",
      "Epoch 6/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4397 - acc: 0.8519Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 11s - loss: 0.8615 - acc: 0.6233Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "38/38 [==============================] - 26s 676ms/step - loss: 0.4356 - acc: 0.8542 - val_loss: 0.8615 - val_acc: 0.6233\n",
      "Epoch 00006: early stopping\n",
      "--- Starting trial: 3\n",
      "Found 1200 validated image filenames belonging to 2 classes.\n",
      "Found 300 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.6778 - acc: 0.6986Epoch 1/20\n",
      "38/38 [==============================] - 31s 820ms/step - loss: 0.6782 - acc: 0.7008 - val_loss: 0.8019 - val_acc: 0.5300\n",
      "Epoch 2/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.5254 - acc: 0.8099Epoch 1/20\n",
      "38/38 [==============================] - 26s 680ms/step - loss: 0.5203 - acc: 0.8117 - val_loss: 0.7507 - val_acc: 0.6333\n",
      "Epoch 3/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4803 - acc: 0.8311Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/38 [======>.......................] - ETA: 12s - loss: 0.8241 - acc: 0.5733\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "38/38 [==============================] - 25s 668ms/step - loss: 0.4777 - acc: 0.8317 - val_loss: 0.8241 - val_acc: 0.5733\n",
      "Epoch 4/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4567 - acc: 0.8416Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 12s - loss: 0.8307 - acc: 0.6000\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "38/38 [==============================] - 25s 668ms/step - loss: 0.4605 - acc: 0.8400 - val_loss: 0.8307 - val_acc: 0.6000\n",
      "Epoch 5/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4373 - acc: 0.8502Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 11s - loss: 0.8945 - acc: 0.5833Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "38/38 [==============================] - 26s 684ms/step - loss: 0.4382 - acc: 0.8492 - val_loss: 0.8945 - val_acc: 0.5833\n",
      "Epoch 00005: early stopping\n",
      "--- Starting trial: 4\n",
      "Found 1200 validated image filenames belonging to 2 classes.\n",
      "Found 300 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.7204 - acc: 0.6764Epoch 1/20\n",
      "38/38 [==============================] - 30s 802ms/step - loss: 0.7133 - acc: 0.6817 - val_loss: 0.8338 - val_acc: 0.5067\n",
      "Epoch 2/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.5124 - acc: 0.8142Epoch 1/20\n",
      "38/38 [==============================] - 26s 672ms/step - loss: 0.5088 - acc: 0.8167 - val_loss: 0.7656 - val_acc: 0.5367\n",
      "Epoch 3/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4710 - acc: 0.8399Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 10s - loss: 0.7897 - acc: 0.5467\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "38/38 [==============================] - 26s 675ms/step - loss: 0.4689 - acc: 0.8408 - val_loss: 0.7897 - val_acc: 0.5467\n",
      "Epoch 4/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4590 - acc: 0.8485Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 10s - loss: 0.8413 - acc: 0.5333\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "38/38 [==============================] - 25s 661ms/step - loss: 0.4630 - acc: 0.8475 - val_loss: 0.8413 - val_acc: 0.5333\n",
      "Epoch 5/20\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.4636 - acc: 0.8450Epoch 1/20\n",
      "10/38 [======>.......................] - ETA: 11s - loss: 0.9554 - acc: 0.5200Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "38/38 [==============================] - 26s 687ms/step - loss: 0.4618 - acc: 0.8467 - val_loss: 0.9554 - val_acc: 0.5200\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "# perform HP tuning and CV on the model on the data\n",
    "Y = train_df['class']\n",
    "trained_models = []\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "    for dropout_rate in HP_DROPOUT.domain.values:\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "            for learning_rate in HP_L_RATE.domain.values:\n",
    "                \n",
    "                skf = StratifiedKFold(n_splits = K_FOLDS, shuffle = True) \n",
    "                k_i = 0\n",
    "                k_fold_models = []\n",
    "                elapsed_times = []\n",
    "                hparams = {\n",
    "                    HP_NUM_UNITS: num_units,\n",
    "                    HP_DROPOUT: dropout_rate,\n",
    "                    HP_OPTIMIZER: optimizer,\n",
    "                    HP_L_RATE: learning_rate\n",
    "                }\n",
    "                print('HPARAMS:',{h.name: hparams[h] for h in hparams})\n",
    "                \n",
    "                for train_index, val_index in skf.split(np.zeros(len(Y)),Y):\n",
    "                    print('--- Starting trial: %s' % k_i)\n",
    "                    t = time.process_time()\n",
    "                    train_gen, valid_gen = preprocess_images_cv(train_index, val_index)\n",
    "\n",
    "                    logdir = os.path.join('trained classifiers','logs','fit', get_model_name(k_i))\n",
    "                    hpdir = os.path.join('trained classifiers','logs','hparam_tuning', get_model_name(k_i))\n",
    "                    modeldir = os.path.join('trained classifiers',str(get_model_name(k_i)+\".h5\"))\n",
    "                    callbacks = [\n",
    "                        keras.callbacks.ModelCheckpoint(modeldir, save_best_only=True, verbose = 0),\n",
    "                        keras.callbacks.EarlyStopping(patience=3, monitor='val_loss', verbose=1, restore_best_weights=True),\n",
    "                        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, verbose=1),\n",
    "                        keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "                      ]\n",
    "                   \n",
    "                    hp.hparams(hparams)  # record the values used in this trial\n",
    "                    model = create_model(hparams)\n",
    "                    model.fit(train_gen, validation_data=valid_gen, epochs=MAX_EPOCHS, callbacks=callbacks)\n",
    "                    elapsed_times.append(time.process_time() - t)\n",
    "                    k_fold_models.append(modeldir)\n",
    "                    \n",
    "                    del model, train_gen, valid_gen\n",
    "                    keras.backend.clear_session()\n",
    "                    k_i += 1\n",
    "                    \n",
    "                trained_models.append([hparams, k_fold_models, elapsed_times])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d6530",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9851f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in trained_models:\n",
    "    model = create_model(j[0])\n",
    "    accuracies = []\n",
    "    k_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    tp = []\n",
    "    fp = []\n",
    "    tn = []\n",
    "    fn = []\n",
    "    for i in j[1]:\n",
    "        model.load_weights(i)\n",
    "        test_pred = model.predict(test_gen)\n",
    "        y_pred = np.rint(test_pred).flatten()\n",
    "        y_test= [test_gen.class_indices[k] for k in test_df['class'].values.tolist()]\n",
    "        k_accuracies.append(model.evaluate(test_gen)[1])\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "        recalls.append(recall_score(y_test, y_pred))\n",
    "        f1s.append(f1_score(y_test, y_pred))\n",
    "        tn_x, fp_x, fn_x, tp_x = confusion_matrix(y_test, y_pred).ravel()\n",
    "        tp.append(tp_x)\n",
    "        fp.append(fp_x)\n",
    "        tn.append(tn_x)\n",
    "        fn.append(fn_x)\n",
    "\n",
    "    print(\"SCORE FOR HPARAMS:\", {h.name: j[0][h] for h in j[0]})\n",
    "    print(\"Mean accuracy:\", mean(accuracies))\n",
    "    print(\"Mean k_accuracy;\", mean(k_accuracies))\n",
    "    print(\"Mean precision:\", mean(precisions))\n",
    "    print(\"Mean recall:\", mean(recalls))\n",
    "    print(\"Mean f1:\", mean(f1s))\n",
    "    print(\"Mean training time:\", mean(j[2]))\n",
    "    hdict = {h.name: j[0][h] for h in j[0]}\n",
    "    df =  pd.DataFrame(np.array([[str(j[1]),\n",
    "                                  hdict.get('num_units'), hdict.get('dropout'), hdict.get('optimizer'), \n",
    "                                  hdict.get('learning_rate'),IMAGE_SIZE, BATCH_SIZE, DATASET_SIZE, MAX_EPOCHS, K_FOLDS,\n",
    "                                  mean(accuracies), mean(k_accuracies), mean(precisions), mean(recalls), mean(f1s),\n",
    "                                  mean(tn), mean(fp), mean(fn), mean(tp), mean(j[2])\n",
    "                                 ]]),\n",
    "                       columns=['Model Names','num_units', 'dropout','optimizer','learning_rate',\n",
    "                                'IMAGE_SIZE', 'BATCH_SIZE', 'DATASET_SIZE', 'MAX_EPOCHS',\n",
    "                                'K_FOLDS','Accuracy', 'K_accuracy', 'Precision', 'Recall', 'f1 Score', \n",
    "                                'True Negatives', ' False Positives', 'False Negatives', 'True Positives', 'Training Time'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69b88b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 22s 2s/step - loss: 0.8105 - acc: 0.6325\n",
      "13/13 [==============================] - 20s 2s/step - loss: 1.0022 - acc: 0.5525\n",
      "13/13 [==============================] - 19s 1s/step - loss: 1.0483 - acc: 0.5250\n",
      "13/13 [==============================] - 21s 2s/step - loss: 0.8288 - acc: 0.6025\n",
      "13/13 [==============================] - 19s 1s/step - loss: 1.1023 - acc: 0.5575\n",
      "SCORE FOR HPARAMS: {'num_units': 128, 'dropout': 0.1, 'optimizer': 'adam', 'learning_rate': 0.001}\n",
      "Mean accuracy: 0.574\n",
      "Mean k_accuracy; 0.574\n",
      "Mean precision: 0.7292677133134073\n",
      "Mean recall: 0.278\n",
      "Mean f1: 0.3600167801331423\n",
      "Mean training time: 176.915625\n",
      "13/13 [==============================] - 20s 2s/step - loss: 0.8885 - acc: 0.5025\n",
      "13/13 [==============================] - 20s 2s/step - loss: 0.9578 - acc: 0.4975\n",
      "13/13 [==============================] - 21s 2s/step - loss: 0.8083 - acc: 0.5350\n",
      "13/13 [==============================] - 20s 2s/step - loss: 0.7608 - acc: 0.6225\n",
      "13/13 [==============================] - 19s 1s/step - loss: 0.8611 - acc: 0.5075\n",
      "SCORE FOR HPARAMS: {'num_units': 128, 'dropout': 0.1, 'optimizer': 'sgd', 'learning_rate': 0.001}\n",
      "Mean accuracy: 0.533\n",
      "Mean k_accuracy; 0.533\n",
      "Mean precision: 0.5903147851740264\n",
      "Mean recall: 0.151\n",
      "Mean f1: 0.22411006262819458\n",
      "Mean training time: 187.175\n"
     ]
    }
   ],
   "source": [
    "for j in trained_models:\n",
    "    model = create_model(j[0])\n",
    "    accuracies = []\n",
    "    k_accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    tp = []\n",
    "    fp = []\n",
    "    tn = []\n",
    "    fn = []\n",
    "    for i in j[1]:\n",
    "        model.load_weights(i)\n",
    "        test_pred = model.predict(test_gen)\n",
    "        y_pred = np.rint(test_pred).flatten()\n",
    "        y_test= [test_gen.class_indices[k] for k in test_df['class'].values.tolist()]\n",
    "        k_accuracies.append(model.evaluate(test_gen)[1])\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "        recalls.append(recall_score(y_test, y_pred))\n",
    "        f1s.append(f1_score(y_test, y_pred))\n",
    "        tn_x, fp_x, fn_x, tp_x = confusion_matrix(y_test, y_pred).ravel()\n",
    "        tp.append(tp_x)\n",
    "        fp.append(fp_x)\n",
    "        tn.append(tn_x)\n",
    "        fn.append(fn_x)\n",
    "\n",
    "    print(\"SCORE FOR HPARAMS:\", {h.name: j[0][h] for h in j[0]})\n",
    "    print(\"Mean accuracy:\", mean(accuracies))\n",
    "    print(\"Mean k_accuracy;\", mean(k_accuracies))\n",
    "    print(\"Mean precision:\", mean(precisions))\n",
    "    print(\"Mean recall:\", mean(recalls))\n",
    "    print(\"Mean f1:\", mean(f1s))\n",
    "    print(\"Mean training time:\", mean(j[2]))\n",
    "    hdict = {h.name: j[0][h] for h in j[0]}\n",
    "    df =  pd.DataFrame(np.array([[str(j[1]),\n",
    "                                  hdict.get('num_units'), hdict.get('dropout'), hdict.get('optimizer'), \n",
    "                                  hdict.get('learning_rate'),IMAGE_SIZE, BATCH_SIZE, DATASET_SIZE, MAX_EPOCHS, K_FOLDS,\n",
    "                                  mean(accuracies), mean(k_accuracies), mean(precisions), mean(recalls), mean(f1s),\n",
    "                                  mean(tn), mean(fp), mean(fn), mean(tp), mean(j[2])\n",
    "                                 ]]),\n",
    "                       columns=['Model Names','num_units', 'dropout','optimizer','learning_rate',\n",
    "                                'IMAGE_SIZE', 'BATCH_SIZE', 'DATASET_SIZE', 'MAX_EPOCHS',\n",
    "                                'K_FOLDS','Accuracy', 'K_accuracy', 'Precision', 'Recall', 'f1 Score', \n",
    "                                'True Negatives', ' False Positives', 'False Negatives', 'True Positives', 'Training Time'])\n",
    "    with open('results/Xception-v2.csv', 'a') as f:\n",
    "        df.to_csv(f, mode='a', header=f.tell()==0, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f74fbe",
   "metadata": {},
   "source": [
    "NOTE: Accuracy and loss graphs can be seen on TensorBoard, run \"%tensorboard --logdir logs --host 0.0.0.0\" to open"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539eff87-2f7e-4abe-95e7-7f74ecabd587",
   "metadata": {},
   "source": [
    "## Save Weights and Results to Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba6790-5471-4d98-9b12-ef2b17669643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil -m cp -r trained-classifiers gs://trained-classifiers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426362af-9979-4874-8401-cc8a6955b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil -m cp -r model-checkpoints gs://trained-classifiers/"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/managed-notebooks:m90"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
